{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13809379,"sourceType":"datasetVersion","datasetId":7841503}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Converting the meta-file into parquet batches to avoid RAM spikes","metadata":{}},{"cell_type":"code","source":"import os\nimport io\nimport polars as pl\nimport gc\n\nITEM_COL = 'parent_asin'\nCHUNK_SIZE = 300_000 \n# ---------------------\n\ndef convert_to_parquet_batches(input_path, output_folder):\n    \"\"\"\n    Reads the JSONL file in large chunks and saves them as Parquet.\n    This solves the 'JSON Parse Spike' memory issue.\n    \"\"\"\n    print(f\"Step 1: Converting {input_path} to Parquet in batches...\")\n    \n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n        \n    chunk_buffer = []\n    batch_num = 0\n    \n    with open(input_path, 'r', encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            chunk_buffer.append(line)\n            \n            # When we hit the chunk size, save and clear memory\n            if len(chunk_buffer) >= CHUNK_SIZE:\n                _save_batch(chunk_buffer, output_folder, batch_num)\n                batch_num += 1\n                chunk_buffer = [] # Reset buffer\n                gc.collect()      # Force memory cleanup\n                print(f\"   Processed {i+1} lines...\")\n                \n        # Save the final remainder\n        if chunk_buffer:\n            _save_batch(chunk_buffer, output_folder, batch_num)\n\n    print(\"Conversion complete.\")\n\ndef _save_batch(lines, folder, batch_num):\n    \"\"\"Helper to write a batch of lines to a parquet file\"\"\"\n    # Convert list of strings to bytes for Polars\n    f = io.BytesIO(\"\".join(lines).encode('utf-8'))\n    \n    # Read using Polars (fast)\n    df = pl.read_ndjson(f, infer_schema_length=None, ignore_errors=True)\n    \n    # Enforce ID types to string to prevent mixing int/str errors\n    df = df.with_columns([\n                pl.col(ITEM_COL).cast(pl.String)\n    ])\n    \n    # Write compressed parquet\n    output_path = os.path.join(folder, f\"part_{batch_num}.parquet\")\n    df.write_parquet(output_path)\n    \n    # Clean up\n    del df\n    del f\n\n\nconvert_to_parquet_batches(\"/kaggle/input/sports-and-outdoors-raw-amazon2023/meta_Sports_and_Outdoors.jsonl/meta_Sports_and_Outdoors.jsonl\",\"/kaggle/working/temp_parquet_parts_meta\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:30:16.555298Z","iopub.execute_input":"2025-11-21T13:30:16.556401Z","iopub.status.idle":"2025-11-21T13:38:08.911026Z","shell.execute_reply.started":"2025-11-21T13:30:16.556331Z","shell.execute_reply":"2025-11-21T13:38:08.909241Z"}},"outputs":[{"name":"stdout","text":"Step 1: Converting /kaggle/input/sports-and-outdoors-raw-amazon2023/meta_Sports_and_Outdoors.jsonl/meta_Sports_and_Outdoors.jsonl to Parquet in batches...\n   Processed 300000 lines...\n   Processed 600000 lines...\n   Processed 900000 lines...\n   Processed 1200000 lines...\n   Processed 1500000 lines...\nConversion complete.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Taking the parquet batches and the 10-core interactions file to reduce the metadata to 10-core","metadata":{}},{"cell_type":"code","source":"import polars as pl\nimport glob\nimport os\n\ndef process_batches_to_jsonl(interaction_path, metadata_folder, output_path):\n    print(\"--- Starting Process ---\")\n    \n    # 1. OPTIMIZED LOAD of Interaction Keys\n    # We only need the 'asin' column. We drop everything else to save RAM.\n    # We also get unique values immediately to speed up the join.\n    print(f\"Loading interaction keys from {interaction_path}...\")\n    try:\n        # Assuming interaction file is Parquet. If CSV, use read_csv.\n        # We use select() immediately to avoid loading other columns.\n        interaction_keys = (\n            pl.read_ndjson(interaction_path)\n            .select(\"parent_asin\")\n            .unique()\n        )\n    except Exception as e:\n        print(f\"Error loading interactions: {e}\")\n        return\n\n    print(f\"loaded {interaction_keys.height} unique items to keep.\")\n\n    # 2. Identify all metadata batches\n    # Finds all files ending in .parquet in the specified folder\n    parquet_files = glob.glob(os.path.join(metadata_folder, \"*.parquet\"))\n    \n    if not parquet_files:\n        print(\"No .parquet files found in the metadata folder.\")\n        return\n\n    print(f\"Found {len(parquet_files)} metadata batches to process.\")\n\n    # 3. Loop, Filter, and Append\n    # We open the output file in 'ab' (Append Binary) mode. \n    # This allows us to write chunk by chunk into one file.\n    total_rows_saved = 0\n    \n    with open(output_path, \"wb\") as f_out:\n        for i, file_path in enumerate(parquet_files):\n            print(f\"Processing batch {i+1}/{len(parquet_files)}: {os.path.basename(file_path)}\")\n            \n            try:\n                # Load the batch\n                df_batch = pl.read_parquet(file_path)\n                \n                # Filter (Semi Join)\n                # Keeps rows in df_batch only if asin is in interaction_keys\n                df_filtered = df_batch.join(interaction_keys, on=\"parent_asin\", how=\"semi\")\n                \n                row_count = df_filtered.height\n                \n                if row_count > 0:\n                    # Write to the open file handle\n                    df_filtered.write_ndjson(f_out)\n                    total_rows_saved += row_count\n                \n                # Explicitly remove dataframe to free up RAM for the next iteration\n                del df_batch\n                del df_filtered\n                \n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n\n    print(\"--- Processing Complete ---\")\n    print(f\"Total rows written to {output_path}: {total_rows_saved}\")\n\n# --- Usage Instructions ---\n# 1. Path to your big interaction file (10-core subset)\ninteraction_file = \"/kaggle/working/sports_reviews_10core.jsonl\" \n\n# 2. Folder containing your split metadata parquet files\nmetadata_temp_folder = \"/kaggle/working/temp_parquet_parts_meta\"\n\n# 3. The final output file name\noutput_jsonl = \"final_metadata_10_core.jsonl\"\n\n# Run the function\nprocess_batches_to_jsonl(interaction_file, metadata_temp_folder, output_jsonl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:47:53.825078Z","iopub.execute_input":"2025-11-21T13:47:53.825397Z","iopub.status.idle":"2025-11-21T13:48:08.809867Z","shell.execute_reply.started":"2025-11-21T13:47:53.825355Z","shell.execute_reply":"2025-11-21T13:48:08.808884Z"}},"outputs":[{"name":"stdout","text":"--- Starting Process ---\nLoading interaction keys from /kaggle/working/sports_reviews_10core.jsonl...\nloaded 17547 unique items to keep.\nFound 6 metadata batches to process.\nProcessing batch 1/6: part_1.parquet\nProcessing batch 2/6: part_3.parquet\nProcessing batch 3/6: part_4.parquet\nProcessing batch 4/6: part_2.parquet\nProcessing batch 5/6: part_5.parquet\nProcessing batch 6/6: part_0.parquet\n--- Processing Complete ---\nTotal rows written to final_metadata_10_core.jsonl: 17547\n","output_type":"stream"}],"execution_count":10}]}