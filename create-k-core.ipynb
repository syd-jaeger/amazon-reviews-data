{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12431497,"sourceType":"datasetVersion","datasetId":7841503}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport polars as pl\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:23:59.970009Z","iopub.execute_input":"2025-11-20T15:23:59.970465Z","iopub.status.idle":"2025-11-20T15:23:59.984459Z","shell.execute_reply.started":"2025-11-20T15:23:59.970426Z","shell.execute_reply":"2025-11-20T15:23:59.983123Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sports-and-outdoors-raw-amazon2023/Sports_and_Outdoors.jsonl\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import polars as pl\nimport os\nimport io\nimport gc\n\n# --- CONFIGURATION ---\nINPUT_FILE = '/kaggle/input/sports-and-outdoors-raw-amazon2023/Sports_and_Outdoors.jsonl'\nTEMP_FOLDER = 'temp_parquet_parts'\nOUTPUT_FILE = 'sports_reviews_10core.jsonl'\n\nUSER_COL = 'user_id'\nITEM_COL = 'parent_asin'\nMIN_INTERACTIONS = 5\n\n# 1 Million rows is safe for 30GB RAM\nCHUNK_SIZE = 1_000_000 \n# ---------------------\n\ndef convert_to_parquet_batches(input_path, output_folder):\n    \"\"\"\n    Reads the JSONL file in large chunks and saves them as Parquet.\n    This solves the 'JSON Parse Spike' memory issue.\n    \"\"\"\n    print(f\"Step 1: Converting {input_path} to Parquet in batches...\")\n    \n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n        \n    chunk_buffer = []\n    batch_num = 0\n    \n    with open(input_path, 'r', encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            chunk_buffer.append(line)\n            \n            # When we hit the chunk size, save and clear memory\n            if len(chunk_buffer) >= CHUNK_SIZE:\n                _save_batch(chunk_buffer, output_folder, batch_num)\n                batch_num += 1\n                chunk_buffer = [] # Reset buffer\n                gc.collect()      # Force memory cleanup\n                print(f\"   Processed {i+1} lines...\")\n                \n        # Save the final remainder\n        if chunk_buffer:\n            _save_batch(chunk_buffer, output_folder, batch_num)\n\n    print(\"Conversion complete.\")\n\ndef _save_batch(lines, folder, batch_num):\n    \"\"\"Helper to write a batch of lines to a parquet file\"\"\"\n    # Convert list of strings to bytes for Polars\n    f = io.BytesIO(\"\".join(lines).encode('utf-8'))\n    \n    # Read using Polars (fast)\n    df = pl.read_ndjson(f, infer_schema_length=None, ignore_errors=True)\n    \n    # Enforce ID types to string to prevent mixing int/str errors\n    df = df.with_columns([\n        pl.col(USER_COL).cast(pl.String),\n        pl.col(ITEM_COL).cast(pl.String)\n    ])\n    \n    # Write compressed parquet\n    output_path = os.path.join(folder, f\"part_{batch_num}.parquet\")\n    df.write_parquet(output_path)\n    \n    # Clean up\n    del df\n    del f\n\ndef main():\n    # 1. Convert JSONL -> Parquet (The Chunked Step)\n    convert_to_parquet_batches(INPUT_FILE, TEMP_FOLDER)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:23:59.986677Z","iopub.execute_input":"2025-11-20T15:23:59.987089Z","iopub.status.idle":"2025-11-20T15:28:51.130718Z","shell.execute_reply.started":"2025-11-20T15:23:59.987046Z","shell.execute_reply":"2025-11-20T15:28:51.129889Z"}},"outputs":[{"name":"stdout","text":"Step 1: Converting /kaggle/input/sports-and-outdoors-raw-amazon2023/Sports_and_Outdoors.jsonl to Parquet in batches...\n   Processed 1000000 lines...\n   Processed 2000000 lines...\n   Processed 3000000 lines...\n   Processed 4000000 lines...\n   Processed 5000000 lines...\n   Processed 6000000 lines...\n   Processed 7000000 lines...\n   Processed 8000000 lines...\n   Processed 9000000 lines...\n   Processed 10000000 lines...\n   Processed 11000000 lines...\n   Processed 12000000 lines...\n   Processed 13000000 lines...\n   Processed 14000000 lines...\n   Processed 15000000 lines...\n   Processed 16000000 lines...\n   Processed 17000000 lines...\n   Processed 18000000 lines...\n   Processed 19000000 lines...\nConversion complete.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# --- CONFIGURATION ---\nTEMP_FOLDER = 'temp_parquet_parts' # The folder where your files already are\nOUTPUT_FILE = 'sports_reviews_5core.jsonl'\nMIN_INTERACTIONS = 5\n\nUSER_COL = 'user_id'\nITEM_COL = 'parent_asin'\n# ---------------------\n\ndef get_safe_columns(folder_path):\n    \"\"\"\n    Peeks at the first parquet file to get all column names,\n    then removes complex columns known to cause schema conflicts.\n    \"\"\"\n    # Find the first file\n    files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.parquet')])\n    if not files:\n        raise FileNotFoundError(\"No parquet files found in temp folder.\")\n    \n    # Read schema of first file (very fast, reads metadata only)\n    # We use scan_parquet to get the schema without loading data\n    schema = pl.scan_parquet(files[0]).schema\n    all_cols = list(schema.keys())\n    \n    # COLUMNS TO EXCLUDE\n    # 'images' is causing your current crash.\n    # 'videos' often causes similar issues.\n    exclude_cols = ['images', 'videos', 'style'] \n    \n    safe_cols = [c for c in all_cols if c not in exclude_cols]\n    \n    print(f\"Total columns detected: {len(all_cols)}\")\n    print(f\"Excluding problematic columns: {exclude_cols}\")\n    print(f\"Keeping {len(safe_cols)} columns.\")\n    \n    return safe_cols\n\ndef main():\n    print(\"Step 2: Resuming Load (with Schema Safety)...\")\n    \n    # 1. Determine which columns are safe to load\n    try:\n        safe_cols = get_safe_columns(TEMP_FOLDER)\n    except Exception as e:\n        print(f\"Error inspecting files: {e}\")\n        return\n\n    # 2. Load only the safe columns\n    print(\"Loading Parquet files into RAM...\")\n    try:\n        # We explicitly pass 'columns=safe_cols' to avoid the SchemaError\n        df = pl.read_parquet(os.path.join(TEMP_FOLDER, \"*.parquet\"), columns=safe_cols)\n    except Exception as e:\n        print(f\"Still failing to load? Error: {e}\")\n        return\n\n    print(f\"Successfully loaded dataset: {len(df)} rows\")\n    \n # 3. Iterative 10-Core Filter\n    print(f\"Starting {MIN_INTERACTIONS}-core processing...\")\n    iteration = 0\n    \n    while True:\n        iteration += 1\n        start_len = len(df)\n        \n        df = df.filter(pl.len().over(USER_COL) >= MIN_INTERACTIONS)\n        df = df.filter(pl.len().over(ITEM_COL) >= MIN_INTERACTIONS)\n        \n        end_len = len(df)\n        print(f\"Iteration {iteration}: {start_len} -> {end_len} rows\")\n        \n        if start_len == end_len:\n            print(\"Convergence reached.\")\n            break\n\n    # 4. Final Stats & Save\n    n_users = df[USER_COL].n_unique()\n    n_items = df[ITEM_COL].n_unique()\n    \n    print(f\"\\nFinal Stats:\")\n    print(f\"Rows: {len(df)}\")\n    print(f\"Users: {n_users}\")\n    print(f\"Items: {n_items}\")\n    \n    print(f\"Saving result to {OUTPUT_FILE}...\")\n    df.write_ndjson(OUTPUT_FILE)\n    print(\"Done!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T15:28:51.131783Z","iopub.execute_input":"2025-11-20T15:28:51.132128Z","iopub.status.idle":"2025-11-20T15:29:14.354833Z","shell.execute_reply.started":"2025-11-20T15:28:51.132089Z","shell.execute_reply":"2025-11-20T15:29:14.353720Z"}},"outputs":[{"name":"stdout","text":"Step 2: Resuming Load (with Schema Safety)...\nTotal columns detected: 10\nExcluding problematic columns: ['images', 'videos', 'style']\nKeeping 9 columns.\nLoading Parquet files into RAM...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3302340826.py:22: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n  schema = pl.scan_parquet(files[0]).schema\n","output_type":"stream"},{"name":"stdout","text":"Successfully loaded dataset: 19595170 rows\nStarting 5-core processing...\nIteration 1: 19595170 -> 4439848 rows\nIteration 2: 4439848 -> 3724066 rows\nIteration 3: 3724066 -> 3639255 rows\nIteration 4: 3639255 -> 3627556 rows\nIteration 5: 3627556 -> 3625829 rows\nIteration 6: 3625829 -> 3625541 rows\nIteration 7: 3625541 -> 3625501 rows\nIteration 8: 3625501 -> 3625477 rows\nIteration 9: 3625477 -> 3625460 rows\nIteration 10: 3625460 -> 3625448 rows\nIteration 11: 3625448 -> 3625444 rows\nIteration 12: 3625444 -> 3625444 rows\nConvergence reached.\n\nFinal Stats:\nRows: 3625444\nUsers: 425812\nItems: 161362\nSaving result to sports_reviews_5core.jsonl...\nDone!\n","output_type":"stream"}],"execution_count":7}]}